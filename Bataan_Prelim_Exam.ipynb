{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "966f5c05",
   "metadata": {},
   "source": [
    "# Prelim Exam\n",
    "#### Submitted by: Franch Lee D. Bataan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc9988be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Libraries needed \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7cf82c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the dataset to the Jupyter Notebook\n",
    "data_train=pd.read_csv('train.csv')\n",
    "data_test=pd.read_csv('test.csv')\n",
    "Output=pd.read_csv('Output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53f7854b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     PassengerId  Survived  Pclass  \\\n",
      "0              1         0       3   \n",
      "1              2         1       1   \n",
      "2              3         1       3   \n",
      "3              4         1       1   \n",
      "4              5         0       3   \n",
      "..           ...       ...     ...   \n",
      "886          887         0       2   \n",
      "887          888         1       1   \n",
      "888          889         0       3   \n",
      "889          890         1       1   \n",
      "890          891         0       3   \n",
      "\n",
      "                                                  Name     Sex   Age  SibSp  \\\n",
      "0                              Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1    Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                               Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3         Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                             Allen, Mr. William Henry    male  35.0      0   \n",
      "..                                                 ...     ...   ...    ...   \n",
      "886                              Montvila, Rev. Juozas    male  27.0      0   \n",
      "887                       Graham, Miss. Margaret Edith  female  19.0      0   \n",
      "888           Johnston, Miss. Catherine Helen \"Carrie\"  female   NaN      1   \n",
      "889                              Behr, Mr. Karl Howell    male  26.0      0   \n",
      "890                                Dooley, Mr. Patrick    male  32.0      0   \n",
      "\n",
      "     Parch            Ticket     Fare Cabin Embarked  \n",
      "0        0         A/5 21171   7.2500   NaN        S  \n",
      "1        0          PC 17599  71.2833   C85        C  \n",
      "2        0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3        0            113803  53.1000  C123        S  \n",
      "4        0            373450   8.0500   NaN        S  \n",
      "..     ...               ...      ...   ...      ...  \n",
      "886      0            211536  13.0000   NaN        S  \n",
      "887      0            112053  30.0000   B42        S  \n",
      "888      2        W./C. 6607  23.4500   NaN        S  \n",
      "889      0            111369  30.0000  C148        C  \n",
      "890      0            370376   7.7500   NaN        Q  \n",
      "\n",
      "[891 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "#Displaying the dataset the will be use in training\n",
    "print(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4aaf6e6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Survived         0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age            177\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             0\n",
       "Cabin          687\n",
       "Embarked         2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking blank values on columns of the dataset\n",
    "data_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f76e88a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Checking blank values along with the Data Type of the labels\n",
    "print(data_train.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "734db6cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Sex', ylabel='count'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#For visualization of data between gender survivor\n",
    "plt.figure(figsize=(7,7))\n",
    "sns.countplot(data=data_train, x='Sex', hue='Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7f929d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Pclass', ylabel='count'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#To show the countplot of the Pclass (Ticket class)\n",
    "plt.figure(figsize=(7,7))\n",
    "sns.countplot(data=data_train, x='Pclass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ca13024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Pclass', ylabel='Age'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "sns.boxplot(data=data_train, x='Pclass', y='Age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "655988ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing missing values in Age by median from boxplot\n",
    "def fit(cols):\n",
    "    Pclass = cols[1]\n",
    "    Age = cols[0]\n",
    "    if pd.isnull(Age):\n",
    "        if Pclass == 1:\n",
    "            return 37\n",
    "        elif Pclass == 2:\n",
    "            return 29\n",
    "        else:\n",
    "            return 22\n",
    "    else:\n",
    "        return Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00607cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=[data_train, data_test]\n",
    "for data in dataset:\n",
    "    data['Age']=data[['Age','Pclass']].apply(fit, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1742653d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing missing values in the Embarked column by letter Z\n",
    "data_train.Embarked.fillna(\"Z\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e120d45b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['male', 'female'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displaying the elements in the column sex\n",
    "data_train['Sex'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c65c2194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['S', 'C', 'Q', 'Z'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displaying the elements in the column Embarked\n",
    "data_train['Embarked'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76bdefd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, 'C85', 'C123', 'E46', 'G6', 'C103', 'D56', 'A6',\n",
       "       'C23 C25 C27', 'B78', 'D33', 'B30', 'C52', 'B28', 'C83', 'F33',\n",
       "       'F G73', 'E31', 'A5', 'D10 D12', 'D26', 'C110', 'B58 B60', 'E101',\n",
       "       'F E69', 'D47', 'B86', 'F2', 'C2', 'E33', 'B19', 'A7', 'C49', 'F4',\n",
       "       'A32', 'B4', 'B80', 'A31', 'D36', 'D15', 'C93', 'C78', 'D35',\n",
       "       'C87', 'B77', 'E67', 'B94', 'C125', 'C99', 'C118', 'D7', 'A19',\n",
       "       'B49', 'D', 'C22 C26', 'C106', 'C65', 'E36', 'C54',\n",
       "       'B57 B59 B63 B66', 'C7', 'E34', 'C32', 'B18', 'C124', 'C91', 'E40',\n",
       "       'T', 'C128', 'D37', 'B35', 'E50', 'C82', 'B96 B98', 'E10', 'E44',\n",
       "       'A34', 'C104', 'C111', 'C92', 'E38', 'D21', 'E12', 'E63', 'A14',\n",
       "       'B37', 'C30', 'D20', 'B79', 'E25', 'D46', 'B73', 'C95', 'B38',\n",
       "       'B39', 'B22', 'C86', 'C70', 'A16', 'C101', 'C68', 'A10', 'E68',\n",
       "       'B41', 'A20', 'D19', 'D50', 'D9', 'A23', 'B50', 'A26', 'D48',\n",
       "       'E58', 'C126', 'B71', 'B51 B53 B55', 'D49', 'B5', 'B20', 'F G63',\n",
       "       'C62 C64', 'E24', 'C90', 'C45', 'E8', 'B101', 'D45', 'C46', 'D30',\n",
       "       'E121', 'D11', 'E77', 'F38', 'B3', 'D6', 'B82 B84', 'D17', 'A36',\n",
       "       'B102', 'B69', 'E49', 'C47', 'D28', 'E17', 'A24', 'C50', 'B42',\n",
       "       'C148'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displaying the elements in the column Cabin\n",
    "data_train['Cabin'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3981527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "New_Cabin = {\"A\":1, \"B\":2, \"C\":3, \"D\":4, \"E\":5, \"F\":6, \"G\":7, \"Z\":8}\n",
    "dataset = [data_train, data_test]\n",
    "\n",
    "for data in dataset:\n",
    "    data['Cabin']= data['Cabin'].fillna(\"Z\")\n",
    "    data['New_Cabin']= data['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\n",
    "    data['New_Cabin']= data['New_Cabin'].map(New_Cabin)\n",
    "    data['New_Cabin']= data['New_Cabin'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aefa25be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>New_Cabin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>Z</td>\n",
       "      <td>S</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>Z</td>\n",
       "      <td>S</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>Z</td>\n",
       "      <td>S</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  New_Cabin  \n",
       "0      0         A/5 21171   7.2500     Z        S        8.0  \n",
       "1      0          PC 17599  71.2833   C85        C        3.0  \n",
       "2      0  STON/O2. 3101282   7.9250     Z        S        8.0  \n",
       "3      0            113803  53.1000  C123        S        3.0  \n",
       "4      0            373450   8.0500     Z        S        8.0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#displaying the colums and rows in the dataset with values\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de9221cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parch is the number of parents / children aboard the Titanic\n",
    "#SibSp is the number of siblings / spouses aboard the Titanic\n",
    "dataset=[data_train, data_test]\n",
    "for data in dataset:\n",
    "    data['Family'] = data['Parch'] + data['SibSp'] \n",
    "    data.loc[data['Family'] ==0,'Individual'] =1\n",
    "    data.loc[data['Family'] > 0,'Individual'] =0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b53c08a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sex = {'male': 1,'female': 0}\n",
    "Embarked = {'C':0,'Q':1,'S':3,'Z':4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3100de2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=[data_train, data_test]\n",
    "for data in dataset:\n",
    "    data['Sex'].replace(Sex, inplace=True)\n",
    "    data['Embarked'].replace(Embarked, inplace=True)\n",
    "    data_test.Fare.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b83b97e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train=data_train.dropna(subset=['Embarked'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0905b872",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting str and float data type into int64\n",
    "dataset=[data_train, data_test]\n",
    "for data in dataset:\n",
    "    data['Age'] = data['Age'].apply(np.int64)\n",
    "    data['Fare'] = data['Fare'].apply(np.int64)\n",
    "    data['Embarked'] = data['Embarked'].apply(np.int64)\n",
    "    data['New_Cabin'] = data['New_Cabin'].apply(np.int64)\n",
    "    data['Individual'] = data['Individual'].apply(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "932a57bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping columns that will nt be use\n",
    "data_train=data_train.drop(columns=[\"PassengerId\",\"Name\",\"Cabin\",\"Ticket\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "144de286",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['Sex'].replace(Sex, inplace=True)\n",
    "data_test['Embarked'].replace(Embarked, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f037e5de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 11 columns):\n",
      " #   Column      Non-Null Count  Dtype\n",
      "---  ------      --------------  -----\n",
      " 0   Survived    891 non-null    int64\n",
      " 1   Pclass      891 non-null    int64\n",
      " 2   Sex         891 non-null    int64\n",
      " 3   Age         891 non-null    int64\n",
      " 4   SibSp       891 non-null    int64\n",
      " 5   Parch       891 non-null    int64\n",
      " 6   Fare        891 non-null    int64\n",
      " 7   Embarked    891 non-null    int64\n",
      " 8   New_Cabin   891 non-null    int64\n",
      " 9   Family      891 non-null    int64\n",
      " 10  Individual  891 non-null    int64\n",
      "dtypes: int64(11)\n",
      "memory usage: 76.7 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Rechecking if the dataframe has missing values and updated for data_train\n",
    "print(data_train.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cafee0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 418 entries, 0 to 417\n",
      "Data columns (total 14 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   PassengerId  418 non-null    int64 \n",
      " 1   Pclass       418 non-null    int64 \n",
      " 2   Name         418 non-null    object\n",
      " 3   Sex          418 non-null    int64 \n",
      " 4   Age          418 non-null    int64 \n",
      " 5   SibSp        418 non-null    int64 \n",
      " 6   Parch        418 non-null    int64 \n",
      " 7   Ticket       418 non-null    object\n",
      " 8   Fare         418 non-null    int64 \n",
      " 9   Cabin        418 non-null    object\n",
      " 10  Embarked     418 non-null    int64 \n",
      " 11  New_Cabin    418 non-null    int64 \n",
      " 12  Family       418 non-null    int64 \n",
      " 13  Individual   418 non-null    int64 \n",
      "dtypes: int64(11), object(3)\n",
      "memory usage: 45.8+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Rechecking if the dataframe has missing values and updated for data_test\n",
    "print(data_test.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c6aab795",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test=data_test.drop(columns=[\"Name\",\"Cabin\",\"Ticket\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a2b12399",
   "metadata": {},
   "outputs": [],
   "source": [
    "X= data_train.drop(\"Survived\", axis=1)\n",
    "y= data_train['Survived'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f04d1136",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X,y, test_size=0.2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7ec59745",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "11c430d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [int(x) for x in np.linspace(start = 10, stop = 80, num = 10)]\n",
    "max_features = ['auto', 'sqrt']\n",
    "max_depth = [2,4]\n",
    "min_samples_split = [2, 5]\n",
    "min_samples_leaf = [1, 2]\n",
    "bootstrap = [True, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "207316be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [10, 17, 25, 33, 41, 48, 56, 64, 72, 80], 'max_features': ['auto', 'sqrt'], 'max_depth': [2, 4], 'min_samples_split': [2, 5], 'min_samples_leaf': [1, 2], 'bootstrap': [True, False]}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "print(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8aa73d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV,cross_val_score\n",
    "rf_Grid = GridSearchCV(estimator = rf, param_grid = param_grid, cv = 10, verbose=2, n_jobs = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bae24956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 320 candidates, totalling 3200 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, estimator=RandomForestClassifier(), n_jobs=4,\n",
       "             param_grid={'bootstrap': [True, False], 'max_depth': [2, 4],\n",
       "                         'max_features': ['auto', 'sqrt'],\n",
       "                         'min_samples_leaf': [1, 2],\n",
       "                         'min_samples_split': [2, 5],\n",
       "                         'n_estimators': [10, 17, 25, 33, 41, 48, 56, 64, 72,\n",
       "                                          80]},\n",
       "             verbose=2)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_Grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fba00889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'max_depth': 4,\n",
       " 'max_features': 'auto',\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'n_estimators': 48}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_Grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e2aa0525",
   "metadata": {},
   "outputs": [],
   "source": [
    "#With RandomizedSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "rf_RandomGrid = RandomizedSearchCV(estimator = rf, param_distributions = param_grid, cv = 10, verbose=2, n_jobs = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3d603e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, estimator=RandomForestClassifier(), n_jobs=4,\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'max_depth': [2, 4],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2],\n",
       "                                        'min_samples_split': [2, 5],\n",
       "                                        'n_estimators': [10, 17, 25, 33, 41, 48,\n",
       "                                                         56, 64, 72, 80]},\n",
       "                   verbose=2)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_RandomGrid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "995f66cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'max_depth': 4,\n",
       " 'max_features': 'auto',\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'n_estimators': 48}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_Grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f7adc532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 33,\n",
       " 'min_samples_split': 2,\n",
       " 'min_samples_leaf': 2,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_depth': 4,\n",
       " 'bootstrap': False}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_RandomGrid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b986278a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy - : 0.847\n",
      "Test Accuracy - : 0.810\n"
     ]
    }
   ],
   "source": [
    "#Checking Accuracy from rf_Grid\n",
    "print (f'Train Accuracy - : {rf_Grid.score(X_train,y_train):.3f}')\n",
    "print (f'Test Accuracy - : {rf_Grid.score(X_test,y_test):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d90438de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy - : 0.843\n",
      "Test Accuracy - : 0.832\n"
     ]
    }
   ],
   "source": [
    "#Checking Accuracy from RandomGrid\n",
    "print (f'Train Accuracy - : {rf_RandomGrid.score(X_train,y_train):.3f}')\n",
    "print (f'Test Accuracy - : {rf_RandomGrid.score(X_test,y_test):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "308134dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83.16"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_new = RandomForestClassifier(n_estimators = 33, min_samples_split = 2, min_samples_leaf= 2, max_features = 'sqrt', max_depth= 4, bootstrap=False) \n",
    "rf_new.fit(X_train, y_train) \n",
    "y_predict= rf_new.predict(X_test)\n",
    "\n",
    "round(rf_new.score(X,y) * 100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b0465c62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=3000)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#logistic Regression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "logistic=sklearn.linear_model.LogisticRegression(max_iter=3000)\n",
    "logistic.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "47c96d33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80.92"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(logistic.score(X,y) * 100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a91a4c0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "       1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,\n",
       "       1, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_new.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "de54eeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural Network\n",
    "#importing needed libraries\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from numpy import loadtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e9ef5995",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining Keras model\n",
    "model=Sequential()\n",
    "model.add(Dense(256, input_shape=(X_train.shape[1],), activation='sigmoid'))\n",
    "model.add(Dense(12, input_shape=(8,), activation='relu')) \n",
    "model.add(Dense(8, activation='relu')) \n",
    "model.add(Dense(1, activation='sigmoid')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1ff414d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compiling the Keras Model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "04493f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "90/90 [==============================] - 1s 2ms/step - loss: 0.6126 - accuracy: 0.6745\n",
      "Epoch 2/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.5881 - accuracy: 0.7059\n",
      "Epoch 3/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.5703 - accuracy: 0.7104\n",
      "Epoch 4/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.5543 - accuracy: 0.7340\n",
      "Epoch 5/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.5372 - accuracy: 0.7306\n",
      "Epoch 6/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.5101 - accuracy: 0.7542\n",
      "Epoch 7/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.4978 - accuracy: 0.7733\n",
      "Epoch 8/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.4849 - accuracy: 0.7823\n",
      "Epoch 9/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.4725 - accuracy: 0.7744\n",
      "Epoch 10/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.4640 - accuracy: 0.7823\n",
      "Epoch 11/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.4508 - accuracy: 0.7957\n",
      "Epoch 12/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.4551 - accuracy: 0.7868\n",
      "Epoch 13/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.4523 - accuracy: 0.7946\n",
      "Epoch 14/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.4512 - accuracy: 0.7991\n",
      "Epoch 15/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.4383 - accuracy: 0.8025\n",
      "Epoch 16/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.4411 - accuracy: 0.8058\n",
      "Epoch 17/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.4536 - accuracy: 0.7957\n",
      "Epoch 18/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.4367 - accuracy: 0.8047\n",
      "Epoch 19/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.4360 - accuracy: 0.8159\n",
      "Epoch 20/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.4356 - accuracy: 0.8058\n",
      "Epoch 21/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.4346 - accuracy: 0.8159\n",
      "Epoch 22/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.4417 - accuracy: 0.8047\n",
      "Epoch 23/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.4236 - accuracy: 0.8148\n",
      "Epoch 24/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.4207 - accuracy: 0.8182\n",
      "Epoch 25/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.4242 - accuracy: 0.8081\n",
      "Epoch 26/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.4294 - accuracy: 0.8171\n",
      "Epoch 27/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.4254 - accuracy: 0.8126\n",
      "Epoch 28/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.4258 - accuracy: 0.8047\n",
      "Epoch 29/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.4176 - accuracy: 0.8081\n",
      "Epoch 30/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.4394 - accuracy: 0.7991\n",
      "Epoch 31/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.4183 - accuracy: 0.8126\n",
      "Epoch 32/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.4092 - accuracy: 0.8171\n",
      "Epoch 33/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.4170 - accuracy: 0.8114\n",
      "Epoch 34/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.4310 - accuracy: 0.8002\n",
      "Epoch 35/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.4195 - accuracy: 0.8204\n",
      "Epoch 36/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.4211 - accuracy: 0.8092\n",
      "Epoch 37/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.4117 - accuracy: 0.8215\n",
      "Epoch 38/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.4164 - accuracy: 0.8171\n",
      "Epoch 39/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.4061 - accuracy: 0.8204\n",
      "Epoch 40/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.4128 - accuracy: 0.8204\n",
      "Epoch 41/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.4083 - accuracy: 0.8182\n",
      "Epoch 42/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.4145 - accuracy: 0.8227\n",
      "Epoch 43/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3998 - accuracy: 0.8350\n",
      "Epoch 44/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.4216 - accuracy: 0.8215\n",
      "Epoch 45/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.4056 - accuracy: 0.8193\n",
      "Epoch 46/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.4043 - accuracy: 0.8294\n",
      "Epoch 47/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.4029 - accuracy: 0.8227\n",
      "Epoch 48/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3992 - accuracy: 0.8272\n",
      "Epoch 49/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.4046 - accuracy: 0.8316\n",
      "Epoch 50/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3963 - accuracy: 0.8215\n",
      "Epoch 51/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.4010 - accuracy: 0.8305\n",
      "Epoch 52/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.4043 - accuracy: 0.8260\n",
      "Epoch 53/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3964 - accuracy: 0.8328\n",
      "Epoch 54/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3834 - accuracy: 0.8361\n",
      "Epoch 55/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3994 - accuracy: 0.8249\n",
      "Epoch 56/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3960 - accuracy: 0.8249\n",
      "Epoch 57/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.4008 - accuracy: 0.8215\n",
      "Epoch 58/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3902 - accuracy: 0.8249\n",
      "Epoch 59/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3931 - accuracy: 0.8260\n",
      "Epoch 60/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3899 - accuracy: 0.8328\n",
      "Epoch 61/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3905 - accuracy: 0.8395\n",
      "Epoch 62/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3934 - accuracy: 0.8361\n",
      "Epoch 63/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3878 - accuracy: 0.8429\n",
      "Epoch 64/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.4240 - accuracy: 0.8328\n",
      "Epoch 65/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.4253 - accuracy: 0.8103\n",
      "Epoch 66/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.4015 - accuracy: 0.8260\n",
      "Epoch 67/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.4009 - accuracy: 0.8204\n",
      "Epoch 68/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3966 - accuracy: 0.8215\n",
      "Epoch 69/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3921 - accuracy: 0.8339\n",
      "Epoch 70/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.4093 - accuracy: 0.8137\n",
      "Epoch 71/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3864 - accuracy: 0.8328\n",
      "Epoch 72/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3883 - accuracy: 0.8395\n",
      "Epoch 73/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3955 - accuracy: 0.8294\n",
      "Epoch 74/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3888 - accuracy: 0.8283\n",
      "Epoch 75/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3900 - accuracy: 0.8305\n",
      "Epoch 76/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.4060 - accuracy: 0.8227\n",
      "Epoch 77/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3973 - accuracy: 0.8215\n",
      "Epoch 78/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3810 - accuracy: 0.8339\n",
      "Epoch 79/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3777 - accuracy: 0.8429\n",
      "Epoch 80/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3857 - accuracy: 0.8272\n",
      "Epoch 81/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3860 - accuracy: 0.8339\n",
      "Epoch 82/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3763 - accuracy: 0.8350\n",
      "Epoch 83/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3830 - accuracy: 0.8294\n",
      "Epoch 84/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3795 - accuracy: 0.8361\n",
      "Epoch 85/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3874 - accuracy: 0.8215\n",
      "Epoch 86/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3806 - accuracy: 0.8350\n",
      "Epoch 87/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3845 - accuracy: 0.8350\n",
      "Epoch 88/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3785 - accuracy: 0.8272\n",
      "Epoch 89/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3785 - accuracy: 0.8406\n",
      "Epoch 90/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3784 - accuracy: 0.8350\n",
      "Epoch 91/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3935 - accuracy: 0.8294\n",
      "Epoch 92/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3821 - accuracy: 0.8316\n",
      "Epoch 93/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3766 - accuracy: 0.8440\n",
      "Epoch 94/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3768 - accuracy: 0.8316\n",
      "Epoch 95/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3964 - accuracy: 0.8182\n",
      "Epoch 96/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3800 - accuracy: 0.8395\n",
      "Epoch 97/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3698 - accuracy: 0.8406\n",
      "Epoch 98/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3819 - accuracy: 0.8418\n",
      "Epoch 99/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.4113 - accuracy: 0.8272\n",
      "Epoch 100/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3757 - accuracy: 0.8350\n",
      "Epoch 101/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3899 - accuracy: 0.8316\n",
      "Epoch 102/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3750 - accuracy: 0.8328\n",
      "Epoch 103/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3739 - accuracy: 0.8350\n",
      "Epoch 104/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3647 - accuracy: 0.8451\n",
      "Epoch 105/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3781 - accuracy: 0.8339\n",
      "Epoch 106/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3750 - accuracy: 0.8339\n",
      "Epoch 107/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3735 - accuracy: 0.8496\n",
      "Epoch 108/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3709 - accuracy: 0.8373\n",
      "Epoch 109/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3720 - accuracy: 0.8429\n",
      "Epoch 110/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3728 - accuracy: 0.8384\n",
      "Epoch 111/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3699 - accuracy: 0.8361\n",
      "Epoch 112/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3784 - accuracy: 0.8373\n",
      "Epoch 113/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3662 - accuracy: 0.8395\n",
      "Epoch 114/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3716 - accuracy: 0.8395\n",
      "Epoch 115/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3733 - accuracy: 0.8384\n",
      "Epoch 116/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3561 - accuracy: 0.8462\n",
      "Epoch 117/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3741 - accuracy: 0.8429\n",
      "Epoch 118/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3593 - accuracy: 0.8462\n",
      "Epoch 119/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3670 - accuracy: 0.8406\n",
      "Epoch 120/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3701 - accuracy: 0.8361\n",
      "Epoch 121/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3682 - accuracy: 0.8451\n",
      "Epoch 122/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3653 - accuracy: 0.8462\n",
      "Epoch 123/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3606 - accuracy: 0.8418\n",
      "Epoch 124/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3619 - accuracy: 0.8429\n",
      "Epoch 125/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3654 - accuracy: 0.8462\n",
      "Epoch 126/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.4054 - accuracy: 0.8171\n",
      "Epoch 127/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3947 - accuracy: 0.8316\n",
      "Epoch 128/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3643 - accuracy: 0.8395\n",
      "Epoch 129/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3600 - accuracy: 0.8451\n",
      "Epoch 130/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3593 - accuracy: 0.8451\n",
      "Epoch 131/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3706 - accuracy: 0.8305\n",
      "Epoch 132/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3556 - accuracy: 0.8530\n",
      "Epoch 133/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3709 - accuracy: 0.8406\n",
      "Epoch 134/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3583 - accuracy: 0.8474\n",
      "Epoch 135/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3533 - accuracy: 0.8507\n",
      "Epoch 136/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3552 - accuracy: 0.8507\n",
      "Epoch 137/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3587 - accuracy: 0.8474\n",
      "Epoch 138/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3601 - accuracy: 0.8485\n",
      "Epoch 139/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3609 - accuracy: 0.8429\n",
      "Epoch 140/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3524 - accuracy: 0.8496\n",
      "Epoch 141/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3518 - accuracy: 0.8395\n",
      "Epoch 142/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3616 - accuracy: 0.8485\n",
      "Epoch 143/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3546 - accuracy: 0.8474\n",
      "Epoch 144/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3526 - accuracy: 0.8507\n",
      "Epoch 145/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3489 - accuracy: 0.8552\n",
      "Epoch 146/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3610 - accuracy: 0.8496\n",
      "Epoch 147/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3518 - accuracy: 0.8496\n",
      "Epoch 148/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3491 - accuracy: 0.8507\n",
      "Epoch 149/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3562 - accuracy: 0.8541\n",
      "Epoch 150/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3628 - accuracy: 0.8485\n",
      "Epoch 151/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3710 - accuracy: 0.8418\n",
      "Epoch 152/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3570 - accuracy: 0.8519\n",
      "Epoch 153/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3471 - accuracy: 0.8429\n",
      "Epoch 154/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3532 - accuracy: 0.8485\n",
      "Epoch 155/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3627 - accuracy: 0.8373\n",
      "Epoch 156/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3435 - accuracy: 0.8496\n",
      "Epoch 157/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3456 - accuracy: 0.8586\n",
      "Epoch 158/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3446 - accuracy: 0.8586\n",
      "Epoch 159/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3421 - accuracy: 0.8653\n",
      "Epoch 160/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3441 - accuracy: 0.8575\n",
      "Epoch 161/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3617 - accuracy: 0.8418\n",
      "Epoch 162/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3461 - accuracy: 0.8519\n",
      "Epoch 163/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3624 - accuracy: 0.8474\n",
      "Epoch 164/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3438 - accuracy: 0.8541\n",
      "Epoch 165/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3576 - accuracy: 0.8552\n",
      "Epoch 166/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3462 - accuracy: 0.8507\n",
      "Epoch 167/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3388 - accuracy: 0.8530\n",
      "Epoch 168/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3614 - accuracy: 0.8485\n",
      "Epoch 169/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3441 - accuracy: 0.8519\n",
      "Epoch 170/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3359 - accuracy: 0.8586\n",
      "Epoch 171/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3433 - accuracy: 0.8507\n",
      "Epoch 172/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3399 - accuracy: 0.8496\n",
      "Epoch 173/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3511 - accuracy: 0.8485\n",
      "Epoch 174/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3475 - accuracy: 0.8507\n",
      "Epoch 175/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3414 - accuracy: 0.8563\n",
      "Epoch 176/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3437 - accuracy: 0.8563\n",
      "Epoch 177/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3354 - accuracy: 0.8620\n",
      "Epoch 178/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3342 - accuracy: 0.8608\n",
      "Epoch 179/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3303 - accuracy: 0.8664\n",
      "Epoch 180/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3481 - accuracy: 0.8485\n",
      "Epoch 181/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3431 - accuracy: 0.8552\n",
      "Epoch 182/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3330 - accuracy: 0.8575\n",
      "Epoch 183/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3309 - accuracy: 0.8563\n",
      "Epoch 184/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3512 - accuracy: 0.8451\n",
      "Epoch 185/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3393 - accuracy: 0.8597\n",
      "Epoch 186/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3318 - accuracy: 0.8676\n",
      "Epoch 187/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3383 - accuracy: 0.8507\n",
      "Epoch 188/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3395 - accuracy: 0.8541\n",
      "Epoch 189/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3367 - accuracy: 0.8530\n",
      "Epoch 190/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3330 - accuracy: 0.8563\n",
      "Epoch 191/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3305 - accuracy: 0.8642\n",
      "Epoch 192/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3312 - accuracy: 0.8653\n",
      "Epoch 193/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3319 - accuracy: 0.8530\n",
      "Epoch 194/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3413 - accuracy: 0.8552\n",
      "Epoch 195/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3285 - accuracy: 0.8586\n",
      "Epoch 196/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3307 - accuracy: 0.8620\n",
      "Epoch 197/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3420 - accuracy: 0.8519\n",
      "Epoch 198/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3291 - accuracy: 0.8608\n",
      "Epoch 199/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3346 - accuracy: 0.8642\n",
      "Epoch 200/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3303 - accuracy: 0.8642\n",
      "Epoch 201/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3355 - accuracy: 0.8530\n",
      "Epoch 202/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3251 - accuracy: 0.8709\n",
      "Epoch 203/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3292 - accuracy: 0.8563\n",
      "Epoch 204/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3242 - accuracy: 0.8608\n",
      "Epoch 205/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3252 - accuracy: 0.8552\n",
      "Epoch 206/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3730 - accuracy: 0.8563\n",
      "Epoch 207/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3460 - accuracy: 0.8485\n",
      "Epoch 208/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3425 - accuracy: 0.8541\n",
      "Epoch 209/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3269 - accuracy: 0.8597\n",
      "Epoch 210/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3385 - accuracy: 0.8586\n",
      "Epoch 211/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3296 - accuracy: 0.8620\n",
      "Epoch 212/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3229 - accuracy: 0.8732\n",
      "Epoch 213/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3237 - accuracy: 0.8642\n",
      "Epoch 214/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3191 - accuracy: 0.8608\n",
      "Epoch 215/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3160 - accuracy: 0.8721\n",
      "Epoch 216/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3298 - accuracy: 0.8597\n",
      "Epoch 217/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3303 - accuracy: 0.8620\n",
      "Epoch 218/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3271 - accuracy: 0.8631\n",
      "Epoch 219/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3222 - accuracy: 0.8653\n",
      "Epoch 220/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3422 - accuracy: 0.8608\n",
      "Epoch 221/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3245 - accuracy: 0.8552\n",
      "Epoch 222/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3164 - accuracy: 0.8608\n",
      "Epoch 223/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3231 - accuracy: 0.8631\n",
      "Epoch 224/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3302 - accuracy: 0.8552\n",
      "Epoch 225/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3193 - accuracy: 0.8653\n",
      "Epoch 226/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3232 - accuracy: 0.8620\n",
      "Epoch 227/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3197 - accuracy: 0.8608\n",
      "Epoch 228/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3127 - accuracy: 0.8765\n",
      "Epoch 229/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3232 - accuracy: 0.8530\n",
      "Epoch 230/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3121 - accuracy: 0.8687\n",
      "Epoch 231/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3150 - accuracy: 0.8642\n",
      "Epoch 232/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3277 - accuracy: 0.8541\n",
      "Epoch 233/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3135 - accuracy: 0.8721\n",
      "Epoch 234/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3158 - accuracy: 0.8530\n",
      "Epoch 235/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3055 - accuracy: 0.8777\n",
      "Epoch 236/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3153 - accuracy: 0.8653\n",
      "Epoch 237/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3132 - accuracy: 0.8676\n",
      "Epoch 238/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3201 - accuracy: 0.8642\n",
      "Epoch 239/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3145 - accuracy: 0.8743\n",
      "Epoch 240/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3232 - accuracy: 0.8653\n",
      "Epoch 241/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3216 - accuracy: 0.8631\n",
      "Epoch 242/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3338 - accuracy: 0.8642\n",
      "Epoch 243/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3094 - accuracy: 0.8788\n",
      "Epoch 244/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3140 - accuracy: 0.8664\n",
      "Epoch 245/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3114 - accuracy: 0.8743\n",
      "Epoch 246/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3061 - accuracy: 0.8721\n",
      "Epoch 247/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3230 - accuracy: 0.8541\n",
      "Epoch 248/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3091 - accuracy: 0.8732\n",
      "Epoch 249/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3074 - accuracy: 0.8743\n",
      "Epoch 250/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3087 - accuracy: 0.8788\n",
      "Epoch 251/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3124 - accuracy: 0.8732\n",
      "Epoch 252/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3057 - accuracy: 0.8788\n",
      "Epoch 253/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3076 - accuracy: 0.8799\n",
      "Epoch 254/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3093 - accuracy: 0.8709\n",
      "Epoch 255/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3336 - accuracy: 0.8608\n",
      "Epoch 256/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3137 - accuracy: 0.8709\n",
      "Epoch 257/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3076 - accuracy: 0.8810\n",
      "Epoch 258/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3086 - accuracy: 0.8810\n",
      "Epoch 259/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3046 - accuracy: 0.8777\n",
      "Epoch 260/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3134 - accuracy: 0.8676\n",
      "Epoch 261/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3105 - accuracy: 0.8732\n",
      "Epoch 262/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3043 - accuracy: 0.8743\n",
      "Epoch 263/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3051 - accuracy: 0.8754\n",
      "Epoch 264/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2986 - accuracy: 0.8788\n",
      "Epoch 265/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3014 - accuracy: 0.8822\n",
      "Epoch 266/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3045 - accuracy: 0.8799\n",
      "Epoch 267/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3068 - accuracy: 0.8698\n",
      "Epoch 268/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3082 - accuracy: 0.8676\n",
      "Epoch 269/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3023 - accuracy: 0.8754\n",
      "Epoch 270/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3075 - accuracy: 0.8743\n",
      "Epoch 271/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3003 - accuracy: 0.8777\n",
      "Epoch 272/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2935 - accuracy: 0.8833\n",
      "Epoch 273/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3021 - accuracy: 0.8866\n",
      "Epoch 274/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3107 - accuracy: 0.8698\n",
      "Epoch 275/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3042 - accuracy: 0.8676\n",
      "Epoch 276/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3088 - accuracy: 0.8743\n",
      "Epoch 277/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3327 - accuracy: 0.8698\n",
      "Epoch 278/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2992 - accuracy: 0.8822\n",
      "Epoch 279/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3007 - accuracy: 0.8810\n",
      "Epoch 280/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3032 - accuracy: 0.8676\n",
      "Epoch 281/500\n",
      "90/90 [==============================] - 0s 3ms/step - loss: 0.3095 - accuracy: 0.8676\n",
      "Epoch 282/500\n",
      "90/90 [==============================] - 0s 3ms/step - loss: 0.2985 - accuracy: 0.8799\n",
      "Epoch 283/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2950 - accuracy: 0.8799\n",
      "Epoch 284/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3008 - accuracy: 0.8788\n",
      "Epoch 285/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3003 - accuracy: 0.8777\n",
      "Epoch 286/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2988 - accuracy: 0.8788\n",
      "Epoch 287/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2935 - accuracy: 0.8810\n",
      "Epoch 288/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2934 - accuracy: 0.8799\n",
      "Epoch 289/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2990 - accuracy: 0.8833\n",
      "Epoch 290/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3179 - accuracy: 0.8687\n",
      "Epoch 291/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2989 - accuracy: 0.8743\n",
      "Epoch 292/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3105 - accuracy: 0.8687\n",
      "Epoch 293/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3025 - accuracy: 0.8799\n",
      "Epoch 294/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2998 - accuracy: 0.8788\n",
      "Epoch 295/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3123 - accuracy: 0.8698\n",
      "Epoch 296/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3083 - accuracy: 0.8687\n",
      "Epoch 297/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3075 - accuracy: 0.8698\n",
      "Epoch 298/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3094 - accuracy: 0.8653\n",
      "Epoch 299/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2992 - accuracy: 0.8822\n",
      "Epoch 300/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2918 - accuracy: 0.8810\n",
      "Epoch 301/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2876 - accuracy: 0.8844\n",
      "Epoch 302/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2954 - accuracy: 0.8889\n",
      "Epoch 303/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2935 - accuracy: 0.8833\n",
      "Epoch 304/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3337 - accuracy: 0.8721\n",
      "Epoch 305/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2939 - accuracy: 0.8833\n",
      "Epoch 306/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2862 - accuracy: 0.8934\n",
      "Epoch 307/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2914 - accuracy: 0.8822\n",
      "Epoch 308/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2878 - accuracy: 0.8799\n",
      "Epoch 309/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3112 - accuracy: 0.8754\n",
      "Epoch 310/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2829 - accuracy: 0.8822\n",
      "Epoch 311/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2945 - accuracy: 0.8855\n",
      "Epoch 312/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2876 - accuracy: 0.8822\n",
      "Epoch 313/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2959 - accuracy: 0.8788\n",
      "Epoch 314/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2886 - accuracy: 0.8878\n",
      "Epoch 315/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2995 - accuracy: 0.8721\n",
      "Epoch 316/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2872 - accuracy: 0.8765\n",
      "Epoch 317/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2869 - accuracy: 0.8844\n",
      "Epoch 318/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2923 - accuracy: 0.8822\n",
      "Epoch 319/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2853 - accuracy: 0.8833\n",
      "Epoch 320/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2889 - accuracy: 0.8855\n",
      "Epoch 321/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2863 - accuracy: 0.8889\n",
      "Epoch 322/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3037 - accuracy: 0.8676\n",
      "Epoch 323/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2892 - accuracy: 0.8810\n",
      "Epoch 324/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2810 - accuracy: 0.8855\n",
      "Epoch 325/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2827 - accuracy: 0.8833\n",
      "Epoch 326/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2867 - accuracy: 0.8833\n",
      "Epoch 327/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3061 - accuracy: 0.8799\n",
      "Epoch 328/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2905 - accuracy: 0.8855\n",
      "Epoch 329/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2899 - accuracy: 0.8754\n",
      "Epoch 330/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2820 - accuracy: 0.8844\n",
      "Epoch 331/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2959 - accuracy: 0.8765\n",
      "Epoch 332/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2852 - accuracy: 0.8788\n",
      "Epoch 333/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3098 - accuracy: 0.8552\n",
      "Epoch 334/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2967 - accuracy: 0.8743\n",
      "Epoch 335/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2956 - accuracy: 0.8833\n",
      "Epoch 336/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2969 - accuracy: 0.8833\n",
      "Epoch 337/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2998 - accuracy: 0.8777\n",
      "Epoch 338/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3063 - accuracy: 0.8721\n",
      "Epoch 339/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2969 - accuracy: 0.8822\n",
      "Epoch 340/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2975 - accuracy: 0.8698\n",
      "Epoch 341/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3119 - accuracy: 0.8732\n",
      "Epoch 342/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2856 - accuracy: 0.8844\n",
      "Epoch 343/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2899 - accuracy: 0.8788\n",
      "Epoch 344/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2942 - accuracy: 0.8833\n",
      "Epoch 345/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2910 - accuracy: 0.8844\n",
      "Epoch 346/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2943 - accuracy: 0.8687\n",
      "Epoch 347/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2864 - accuracy: 0.8855\n",
      "Epoch 348/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2894 - accuracy: 0.8866\n",
      "Epoch 349/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2949 - accuracy: 0.8777\n",
      "Epoch 350/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2868 - accuracy: 0.8844\n",
      "Epoch 351/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2906 - accuracy: 0.8822\n",
      "Epoch 352/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2838 - accuracy: 0.8822\n",
      "Epoch 353/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2855 - accuracy: 0.8878\n",
      "Epoch 354/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2894 - accuracy: 0.8833\n",
      "Epoch 355/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2905 - accuracy: 0.8833\n",
      "Epoch 356/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2877 - accuracy: 0.8833\n",
      "Epoch 357/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3114 - accuracy: 0.8788\n",
      "Epoch 358/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2952 - accuracy: 0.8765\n",
      "Epoch 359/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2795 - accuracy: 0.8810\n",
      "Epoch 360/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2822 - accuracy: 0.8844\n",
      "Epoch 361/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2880 - accuracy: 0.8855\n",
      "Epoch 362/500\n",
      "90/90 [==============================] - 0s 3ms/step - loss: 0.2959 - accuracy: 0.8721\n",
      "Epoch 363/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2871 - accuracy: 0.8878\n",
      "Epoch 364/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2803 - accuracy: 0.8878\n",
      "Epoch 365/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3011 - accuracy: 0.8878\n",
      "Epoch 366/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2865 - accuracy: 0.8777\n",
      "Epoch 367/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2822 - accuracy: 0.8822\n",
      "Epoch 368/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2889 - accuracy: 0.8833\n",
      "Epoch 369/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2901 - accuracy: 0.8855\n",
      "Epoch 370/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2741 - accuracy: 0.8956\n",
      "Epoch 371/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2991 - accuracy: 0.8777\n",
      "Epoch 372/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2866 - accuracy: 0.8889\n",
      "Epoch 373/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2852 - accuracy: 0.8844\n",
      "Epoch 374/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2734 - accuracy: 0.8934\n",
      "Epoch 375/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2704 - accuracy: 0.8923\n",
      "Epoch 376/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2706 - accuracy: 0.8844\n",
      "Epoch 377/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2715 - accuracy: 0.8923\n",
      "Epoch 378/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2674 - accuracy: 0.8979\n",
      "Epoch 379/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2712 - accuracy: 0.8923\n",
      "Epoch 380/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2788 - accuracy: 0.8923\n",
      "Epoch 381/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2837 - accuracy: 0.8866\n",
      "Epoch 382/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2712 - accuracy: 0.8990\n",
      "Epoch 383/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2737 - accuracy: 0.8866\n",
      "Epoch 384/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2718 - accuracy: 0.8934\n",
      "Epoch 385/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2709 - accuracy: 0.8934\n",
      "Epoch 386/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2826 - accuracy: 0.8878\n",
      "Epoch 387/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2668 - accuracy: 0.8911\n",
      "Epoch 388/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2696 - accuracy: 0.8945\n",
      "Epoch 389/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2807 - accuracy: 0.8878\n",
      "Epoch 390/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2922 - accuracy: 0.8855\n",
      "Epoch 391/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2944 - accuracy: 0.8844\n",
      "Epoch 392/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2677 - accuracy: 0.8979\n",
      "Epoch 393/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2732 - accuracy: 0.8923\n",
      "Epoch 394/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2628 - accuracy: 0.9001\n",
      "Epoch 395/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2762 - accuracy: 0.8889\n",
      "Epoch 396/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2667 - accuracy: 0.8889\n",
      "Epoch 397/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2783 - accuracy: 0.8967\n",
      "Epoch 398/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2767 - accuracy: 0.8889\n",
      "Epoch 399/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2737 - accuracy: 0.8878\n",
      "Epoch 400/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2665 - accuracy: 0.8945\n",
      "Epoch 401/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2605 - accuracy: 0.8923\n",
      "Epoch 402/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2657 - accuracy: 0.8900\n",
      "Epoch 403/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2802 - accuracy: 0.8810\n",
      "Epoch 404/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2638 - accuracy: 0.8956\n",
      "Epoch 405/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2646 - accuracy: 0.8945\n",
      "Epoch 406/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2643 - accuracy: 0.8900\n",
      "Epoch 407/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2781 - accuracy: 0.8866\n",
      "Epoch 408/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2665 - accuracy: 0.8934\n",
      "Epoch 409/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3151 - accuracy: 0.8721\n",
      "Epoch 410/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2779 - accuracy: 0.8945\n",
      "Epoch 411/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2678 - accuracy: 0.8923\n",
      "Epoch 412/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2677 - accuracy: 0.8945\n",
      "Epoch 413/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2685 - accuracy: 0.8956\n",
      "Epoch 414/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2755 - accuracy: 0.8855\n",
      "Epoch 415/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2608 - accuracy: 0.9001\n",
      "Epoch 416/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2886 - accuracy: 0.8754\n",
      "Epoch 417/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2702 - accuracy: 0.8844\n",
      "Epoch 418/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2604 - accuracy: 0.8979\n",
      "Epoch 419/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2618 - accuracy: 0.8967\n",
      "Epoch 420/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2587 - accuracy: 0.8979\n",
      "Epoch 421/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2559 - accuracy: 0.9001\n",
      "Epoch 422/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2772 - accuracy: 0.8866\n",
      "Epoch 423/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2632 - accuracy: 0.8878\n",
      "Epoch 424/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2631 - accuracy: 0.8967\n",
      "Epoch 425/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2564 - accuracy: 0.8956\n",
      "Epoch 426/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2611 - accuracy: 0.8990\n",
      "Epoch 427/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2648 - accuracy: 0.9012\n",
      "Epoch 428/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2744 - accuracy: 0.8866\n",
      "Epoch 429/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.3005 - accuracy: 0.8754\n",
      "Epoch 430/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2773 - accuracy: 0.8923\n",
      "Epoch 431/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2619 - accuracy: 0.8967\n",
      "Epoch 432/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2613 - accuracy: 0.8900\n",
      "Epoch 433/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2665 - accuracy: 0.8911\n",
      "Epoch 434/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2580 - accuracy: 0.8967\n",
      "Epoch 435/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2528 - accuracy: 0.9024\n",
      "Epoch 436/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2627 - accuracy: 0.8923\n",
      "Epoch 437/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2958 - accuracy: 0.8765\n",
      "Epoch 438/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2640 - accuracy: 0.8979\n",
      "Epoch 439/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2650 - accuracy: 0.9024\n",
      "Epoch 440/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2907 - accuracy: 0.8833\n",
      "Epoch 441/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2811 - accuracy: 0.8833\n",
      "Epoch 442/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2560 - accuracy: 0.8979\n",
      "Epoch 443/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2723 - accuracy: 0.8956\n",
      "Epoch 444/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2507 - accuracy: 0.8990\n",
      "Epoch 445/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2694 - accuracy: 0.8900\n",
      "Epoch 446/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2605 - accuracy: 0.8956\n",
      "Epoch 447/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2587 - accuracy: 0.8956\n",
      "Epoch 448/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2559 - accuracy: 0.8967\n",
      "Epoch 449/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2731 - accuracy: 0.8967\n",
      "Epoch 450/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2616 - accuracy: 0.8967\n",
      "Epoch 451/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2508 - accuracy: 0.8956\n",
      "Epoch 452/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2656 - accuracy: 0.8979\n",
      "Epoch 453/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2657 - accuracy: 0.8923\n",
      "Epoch 454/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2612 - accuracy: 0.8911\n",
      "Epoch 455/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2567 - accuracy: 0.9012\n",
      "Epoch 456/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2495 - accuracy: 0.9012\n",
      "Epoch 457/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2566 - accuracy: 0.8979\n",
      "Epoch 458/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2515 - accuracy: 0.9001\n",
      "Epoch 459/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2604 - accuracy: 0.8923\n",
      "Epoch 460/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2627 - accuracy: 0.8945\n",
      "Epoch 461/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2620 - accuracy: 0.8923\n",
      "Epoch 462/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2511 - accuracy: 0.8945\n",
      "Epoch 463/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2535 - accuracy: 0.8979\n",
      "Epoch 464/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2548 - accuracy: 0.9001\n",
      "Epoch 465/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2674 - accuracy: 0.8979\n",
      "Epoch 466/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2500 - accuracy: 0.8979\n",
      "Epoch 467/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2590 - accuracy: 0.8889\n",
      "Epoch 468/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2499 - accuracy: 0.9080\n",
      "Epoch 469/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2683 - accuracy: 0.8956\n",
      "Epoch 470/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2589 - accuracy: 0.8956\n",
      "Epoch 471/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2845 - accuracy: 0.8866\n",
      "Epoch 472/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2557 - accuracy: 0.8990\n",
      "Epoch 473/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2730 - accuracy: 0.8956\n",
      "Epoch 474/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2755 - accuracy: 0.8900\n",
      "Epoch 475/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2748 - accuracy: 0.8990\n",
      "Epoch 476/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2553 - accuracy: 0.8934\n",
      "Epoch 477/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2507 - accuracy: 0.9001\n",
      "Epoch 478/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2619 - accuracy: 0.8934\n",
      "Epoch 479/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2434 - accuracy: 0.9035\n",
      "Epoch 480/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2439 - accuracy: 0.9035\n",
      "Epoch 481/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2504 - accuracy: 0.8967\n",
      "Epoch 482/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2471 - accuracy: 0.9035\n",
      "Epoch 483/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2532 - accuracy: 0.8911\n",
      "Epoch 484/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2481 - accuracy: 0.9035\n",
      "Epoch 485/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2578 - accuracy: 0.8956\n",
      "Epoch 486/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2538 - accuracy: 0.8979\n",
      "Epoch 487/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2531 - accuracy: 0.9057\n",
      "Epoch 488/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2447 - accuracy: 0.8956\n",
      "Epoch 489/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2465 - accuracy: 0.9024\n",
      "Epoch 490/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2533 - accuracy: 0.8956\n",
      "Epoch 491/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2547 - accuracy: 0.8979\n",
      "Epoch 492/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2454 - accuracy: 0.9158\n",
      "Epoch 493/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2453 - accuracy: 0.8945\n",
      "Epoch 494/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2410 - accuracy: 0.9024\n",
      "Epoch 495/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2426 - accuracy: 0.9035\n",
      "Epoch 496/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2474 - accuracy: 0.8979\n",
      "Epoch 497/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2457 - accuracy: 0.9001\n",
      "Epoch 498/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2622 - accuracy: 0.8979\n",
      "Epoch 499/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2644 - accuracy: 0.8889\n",
      "Epoch 500/500\n",
      "90/90 [==============================] - 0s 2ms/step - loss: 0.2540 - accuracy: 0.8945\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f7ab7a3fa0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fitting the keras model\n",
    "model.fit(X,y, epochs=500, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ec26194b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 0s 1ms/step - loss: 0.2415 - accuracy: 0.9080\n",
      "Accuracy: 90.80\n"
     ]
    }
   ],
   "source": [
    "#Evaluating the model\n",
    "_,accuracy = model.evaluate(X, y)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e417ae63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 0s 1ms/step\n",
      "28/28 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Making Predictions using the model\n",
    "predictions = model.predict(X)\n",
    "rounded = [round(x[0]) for x in predictions]\n",
    "predictions = (model.predict(X) > 0.5).astype(int)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c3cb1a",
   "metadata": {},
   "source": [
    "# Comparison\n",
    "##### The activity involves the prediction of whether the people have survived from the Titanic incident, given the two datasets namely, \"train.csv\" and “test.csv”. Using two machine learning algorithms and a neural network, predicting the survivability of the people in the mentioned incident becomes possibe.\n",
    "\n",
    "###### Logistic Regression and Random Forest Classifier are the two machine learning algorithms used in this activity. Both the machine learning algorithms provided an accuracy level of higher than 80 percent. Comparing the accuracy level of the machine learning algorithms based on the given dataset, the Random Forrest provided a better accuracy level with 83 percent in terms of prediction while 80 percent is for the Logistic Regression.  Logistic regression is categorized as a supervised learning method that is mostly used in binary classification. In the activity, the algorithm is used as the is already prepared were no missing values, and all labels have the same data type. In terms of the speed of provisioning an output, I observed that the Logical regression provides faster results than the Random Forrest. The logistic regression predicts data in terms of probability and it predicts whether the output is true or false. Random Forrest provides better accuracy but slower performance. It is a collection of multiple random decision trees and is much less sensitive to the data. The data is splited base on the number of new conditions until all the classes have been separated. The proceeding data will follow the correct path until it reaches the final node which gives us the final result for each tree it from from the sampled data. The neural network, on the other hand, differs from these two machine learning algorithms. It is composed of many layers where each node is composed of input, bias, or weights then to the output. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980fa0a0",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "##### When creating a model, it is crucial to clean the data in the dataset first before using it to train the model and make predictions out of the model it produced. Missing data are sometimes correlated with other elements of the data. In the activity, multiple elements such as in the column of age, cabin, and gender have shown a significant correlation with each other. The survivability of people in the incident found that age and class of the ticket by the people are major contributors to survivability. The models formed through the use of machine learning algorithm and proper dataset helps to predict the survivors with a decent level of accuracy. In neural networks, the accuracy of the model increased as we add more values to the epochs. Over time, the neuron with the highest value determines the output. The predicted output is compared to the other neurons to determine the errors. The data in the neurons that contain errors will then be passed back to the input and weights will be adjusted. The cycle will continue iteratively along with multiple inputs as long as the predicted output becomes accurate. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6210a26e",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "##### In conclusion, Neural Network yield the highest accuracy among the three models. Although, the neural network can give better results by increasing the number of epochs, it requires a longer time to yield a higher accuracy level to give an accurate output to the problem. The logistic regression on the other hand may yield lower accuracy but in terms of speed, it is much faster compared to the two models used. The Random Forrest yield 83 percent of accuracy level with slight difference with the logistic regression model. I also learned that configuring the data set where removing the unnecessary data helps to improve the accuracy get by the models yet, there are still some minimal errors that may still occur due to its correlation with other elements.  The activity helps me to learn more about the different machine-learning algorithms. It also helps me to learn different techniques on how to handle data and ding the relationship between the given elements. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
